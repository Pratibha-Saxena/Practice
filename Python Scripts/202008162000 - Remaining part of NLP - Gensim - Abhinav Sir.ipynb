{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------End Tape------#----------------------------------# gensim\n",
    "# open source NLP library\n",
    "# Use top-acedemic model to perform complex tasks like:\n",
    "# Building doc or word vector topic identification\n",
    "# doc comparison\n",
    "# Gensim is billed as a Natural Language Processing package that does ‘Topic Modeling for Humans’ pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-win_amd64.whl (24.2 MB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gensim) (1.18.5)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-2.1.0.tar.gz (116 kB)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gensim) (1.5.0)\n",
      "Collecting Cython==0.29.14\n",
      "  Downloading Cython-0.29.14-cp37-cp37m-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Collecting boto\n",
      "  Downloading boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting boto3\n",
      "\n",
      "  Downloading boto3-1.14.43-py2.py3-none-any.whl (129 kB)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.18.0,>=1.17.43\n",
      "  Downloading botocore-1.17.43-py2.py3-none-any.whl (6.5 MB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from botocore<1.18.0,>=1.17.43->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-2.1.0-py3-none-any.whl size=110324 sha256=e12177461b0b34228af5848132e48218da95c378abed3884442280f45c30fa4e\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\56\\b5\\6d\\86dbe4f29d4688e5163a8b8c6b740494310040286fca4dc648\n",
      "Successfully built smart-open\n",
      "Installing collected packages: boto, jmespath, docutils, botocore, s3transfer, boto3, smart-open, Cython, gensim\n",
      "  Attempting uninstall: docutils\n",
      "    Found existing installation: docutils 0.16\n",
      "    Uninstalling docutils-0.16:\n",
      "      Successfully uninstalled docutils-0.16\n",
      "Successfully installed Cython-0.29.14 boto-2.49.0 boto3-1.14.43 botocore-1.17.43 docutils-0.15.2 gensim-3.8.3 jmespath-0.10.0 s3transfer-0.3.3 smart-open-2.1.0\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['gensim', 'is', 'a', 'robust', 'open-source', 'vector', 'space', 'modeling', ',', 'like', 'building', 'doc', 'or', 'word', 'vector', ',', 'word', 'vector', 'is', 'multi-dim', 'representation', 'of', 'word'], ['gensim', 'is', 'specifically', 'designed', 'to', 'handle', 'large', 'text', 'collections', ',', 'or', 'text', 'documents'], ['gensim', 'includes', 'implementations', 'of', 'tf-idf', ',', 'random', 'projections', ',', 'word2vec', 'and', 'document2vec', 'algorithms', '.', 'tf-idf', 'is', 'for', 'identifying', 'tokens', 'with', 'important', 'a', 'word', 'is', 'to', 'a', 'document'], ['some', 'of', 'the', 'online', 'algorithms', 'in', 'gensim', 'were', 'also', 'published', 'in', 'the', '2011', 'phd', 'dissertation']]\n"
     ]
    }
   ],
   "source": [
    "#-----Preprocessing-----\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "docs = [\"Gensim is a robust open-source vector space modeling, like building doc or word vector, word vector is multi-dim representation of word \",\n",
    "\"Gensim is specifically designed to handle large text collections, or text documents\",\n",
    "\"Gensim includes implementations of tf-idf, random projections, word2vec and document2vec algorithms. tf-idf is for identifying tokens with important a word is to a document\",\n",
    "\"Some of the online algorithms in Gensim were also published in the 2011 PhD dissertation\"]\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in docs]\n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(51 unique tokens: [',', 'a', 'building', 'doc', 'gensim']...)\n",
      " \n",
      "{',': 0, 'a': 1, 'building': 2, 'doc': 3, 'gensim': 4, 'is': 5, 'like': 6, 'modeling': 7, 'multi-dim': 8, 'of': 9, 'open-source': 10, 'or': 11, 'representation': 12, 'robust': 13, 'space': 14, 'vector': 15, 'word': 16, 'collections': 17, 'designed': 18, 'documents': 19, 'handle': 20, 'large': 21, 'specifically': 22, 'text': 23, 'to': 24, '.': 25, 'algorithms': 26, 'and': 27, 'document': 28, 'document2vec': 29, 'for': 30, 'identifying': 31, 'implementations': 32, 'important': 33, 'includes': 34, 'projections': 35, 'random': 36, 'tf-idf': 37, 'tokens': 38, 'with': 39, 'word2vec': 40, '2011': 41, 'also': 42, 'dissertation': 43, 'in': 44, 'online': 45, 'phd': 46, 'published': 47, 'some': 48, 'the': 49, 'were': 50}\n"
     ]
    }
   ],
   "source": [
    "#--------------------gensim dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "print(dictionary)\n",
    "print(' ')\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 3), (16, 3)], [(0, 1), (4, 1), (5, 1), (11, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 2), (24, 1)], [(0, 2), (1, 2), (4, 1), (5, 2), (9, 1), (16, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 2), (38, 1), (39, 1), (40, 1)], [(4, 1), (9, 1), (26, 1), (41, 1), (42, 1), (43, 1), (44, 2), (45, 1), (46, 1), (47, 1), (48, 1), (49, 2), (50, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# build a gensim corpus\n",
    "# Convert document (a list of words) into the bag-of-words\n",
    "# format = list of (token_id, token_count) 2-tuples.\n",
    "# Each word is assumed to be a tokenized and normalized string\n",
    "# (either unicode or utf8-encoded).\n",
    "# No further preprocessing is done on the words in document;\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_docs]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=4, num_nnz=65)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "tfidf = TfidfModel(corpus)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 3), (16, 3)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.09027338834702432), (1, 0.1087532915747138), (2, 0.2175065831494276), (3, 0.2175065831494276), (5, 0.09027338834702432), (6, 0.2175065831494276), (7, 0.2175065831494276), (8, 0.2175065831494276), (9, 0.04513669417351216), (10, 0.2175065831494276), (11, 0.1087532915747138), (12, 0.2175065831494276), (13, 0.2175065831494276), (14, 0.2175065831494276), (15, 0.6525197494482828), (16, 0.3262598747241414)]\n"
     ]
    }
   ],
   "source": [
    "tfidf_weights = tfidf[corpus[0]]\n",
    "print(tfidf_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gensim is a robust open-source vector space modeling, like building doc or word vector, word vector is multi-dim representation of word ', 'Gensim is specifically designed to handle large text collections, or text documents', 'Gensim includes implementations of tf-idf, random projections, word2vec and document2vec algorithms. tf-idf is for identifying tokens with important a word is to a document', 'Some of the online algorithms in Gensim were also published in the 2011 PhD dissertation'] \n",
      "\n",
      "\n",
      "[(15, 0.6525197494482828), (16, 0.3262598747241414), (2, 0.2175065831494276), (3, 0.2175065831494276), (6, 0.2175065831494276), (7, 0.2175065831494276), (8, 0.2175065831494276), (10, 0.2175065831494276), (12, 0.2175065831494276), (13, 0.2175065831494276), (14, 0.2175065831494276), (1, 0.1087532915747138), (11, 0.1087532915747138), (0, 0.09027338834702432), (5, 0.09027338834702432), (9, 0.04513669417351216)] \n",
      "\n",
      "vector 0.6525197494482828\n",
      "word 0.3262598747241414\n",
      "building 0.2175065831494276\n",
      "doc 0.2175065831494276\n",
      "like 0.2175065831494276\n"
     ]
    }
   ],
   "source": [
    "print(docs,\"\\n\\n\")\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "print(sorted_tfidf_weights,\"\\n\")\n",
    "\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id),weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sqrt(sum([j**2 for i,j in tfidf_weights]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
