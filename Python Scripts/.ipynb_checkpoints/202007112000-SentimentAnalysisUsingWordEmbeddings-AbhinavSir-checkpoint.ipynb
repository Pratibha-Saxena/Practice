{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'My name is Anthony Gonzalves and I am alone in this world.',\n",
    "    'How many people were there? Sir, two. And you three, even then you returned empty handed',\n",
    "    'It is not only difficult to capture Don, it is impossible',\n",
    "    'In big-big cities, such small-small incidences keep on happening',\n",
    "    'Dog! I will drink your blood',\n",
    "    'The good, the bad and the ugly',\n",
    "    'Small is beautiful',\n",
    "    '''The illiterate of the twenty-first century will not be those who \n",
    "    cannot learn but those who cannot learn, unlearn and relearn'''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = Tokenizer(num_words = 100, filters = '.', \n",
    "#                     lower = True, split =' ',\n",
    "#                     char_level = False, oov_token = \"<UNK>\")\n",
    "tokenizer = Tokenizer(num_words = 100, lower = True, \n",
    "                     char_level = False, oov_token = \"<UNK>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index:\n",
      " {'<UNK>': 1, 'the': 2, 'is': 3, 'and': 4, 'small': 5, 'i': 6, 'in': 7, 'you': 8, 'it': 9, 'not': 10, 'big': 11, 'will': 12, 'those': 13, 'who': 14, 'cannot': 15, 'learn': 16, 'my': 17, 'name': 18, 'anthony': 19, 'gonzalves': 20, 'am': 21, 'alone': 22, 'this': 23, 'world': 24, 'how': 25, 'many': 26, 'people': 27, 'were': 28, 'there': 29, 'sir': 30, 'two': 31, 'three': 32, 'even': 33, 'then': 34, 'returned': 35, 'empty': 36, 'handed': 37, 'only': 38, 'difficult': 39, 'to': 40, 'capture': 41, 'don': 42, 'impossible': 43, 'cities': 44, 'such': 45, 'incidences': 46, 'keep': 47, 'on': 48, 'happening': 49, 'dog': 50, 'drink': 51, 'your': 52, 'blood': 53, 'good': 54, 'bad': 55, 'ugly': 56, 'beautiful': 57, 'illiterate': 58, 'of': 59, 'twenty': 60, 'first': 61, 'century': 62, 'be': 63, 'but': 64, 'unlearn': 65, 'relearn': 66}\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print('word_index:\\n', word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_count:\n",
      " OrderedDict([('my', 1), ('name', 1), ('is', 4), ('anthony', 1), ('gonzalves', 1), ('and', 4), ('i', 2), ('am', 1), ('alone', 1), ('in', 2), ('this', 1), ('world', 1), ('how', 1), ('many', 1), ('people', 1), ('were', 1), ('there', 1), ('sir', 1), ('two', 1), ('you', 2), ('three', 1), ('even', 1), ('then', 1), ('returned', 1), ('empty', 1), ('handed', 1), ('it', 2), ('not', 2), ('only', 1), ('difficult', 1), ('to', 1), ('capture', 1), ('don', 1), ('impossible', 1), ('big', 2), ('cities', 1), ('such', 1), ('small', 3), ('incidences', 1), ('keep', 1), ('on', 1), ('happening', 1), ('dog', 1), ('will', 2), ('drink', 1), ('your', 1), ('blood', 1), ('the', 5), ('good', 1), ('bad', 1), ('ugly', 1), ('beautiful', 1), ('illiterate', 1), ('of', 1), ('twenty', 1), ('first', 1), ('century', 1), ('be', 1), ('those', 2), ('who', 2), ('cannot', 2), ('learn', 2), ('but', 1), ('unlearn', 1), ('relearn', 1)])\n"
     ]
    }
   ],
   "source": [
    "n = tokenizer.word_counts\n",
    "print('word_count:\\n',n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17, 18, 3, 19, 20, 4, 6, 21, 22, 7, 23, 24], [25, 26, 27, 28, 29, 30, 31, 4, 8, 32, 33, 34, 8, 35, 36, 37], [9, 3, 10, 38, 39, 40, 41, 42, 9, 3, 43], [7, 11, 11, 44, 45, 5, 5, 46, 47, 48, 49], [50, 6, 12, 51, 52, 53], [2, 54, 2, 55, 4, 2, 56], [5, 3, 57], [2, 58, 59, 2, 60, 61, 62, 12, 10, 63, 13, 14, 15, 16, 64, 13, 14, 15, 16, 65, 4, 66]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my name is anthony gonzalves and i am alone in this world', 'how many people were there sir two and you three even then you returned empty handed', 'it is not only difficult to capture don it is impossible', 'in big big cities such small small incidences keep on happening', 'dog i will drink your blood', 'the good the bad and the ugly', 'small is beautiful', 'the illiterate of the twenty first century will not be those who cannot learn but those who cannot learn unlearn and relearn']\n"
     ]
    }
   ],
   "source": [
    "return_sentences = tokenizer.sequences_to_texts(sequences)\n",
    "print(return_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19, 20, 3, 42], [1, 8, 1, 1, 1, 23, 18]]\n",
      "['anthony gonzalves is don', '<UNK> you <UNK> <UNK> <UNK> this name']\n"
     ]
    }
   ],
   "source": [
    "sentences1 = ['Anthony Gonzalves is Don',\n",
    "             'Rahul! you must have heard this name']\n",
    "sequences1 = tokenizer.texts_to_sequences(sentences1)\n",
    "return_sentences1 = tokenizer.sequences_to_texts(sequences1)\n",
    "print(sequences1)\n",
    "print(return_sentences1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17, 18, 3, 19, 20, 4, 6, 21, 22, 7, 23, 24], [25, 26, 27, 28, 29, 30, 31, 4, 8, 32, 33, 34, 8, 35, 36, 37], [9, 3, 10, 38, 39, 40, 41, 42, 9, 3, 43], [7, 11, 11, 44, 45, 5, 5, 46, 47, 48, 49], [50, 6, 12, 51, 52, 53], [2, 54, 2, 55, 4, 2, 56], [5, 3, 57], [2, 58, 59, 2, 60, 61, 62, 12, 10, 63, 13, 14, 15, 16, 64, 13, 14, 15, 16, 65, 4, 66]]\n",
      " \n",
      "[[ 0  0  0  0  0  0  0  0  0  0 17 18  3 19 20  4  6 21 22  7 23 24]\n",
      " [ 0  0  0  0  0  0 25 26 27 28 29 30 31  4  8 32 33 34  8 35 36 37]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  9  3 10 38 39 40 41 42  9  3 43]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  7 11 11 44 45  5  5 46 47 48 49]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 50  6 12 51 52 53]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2 54  2 55  4  2 56]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  3 57]\n",
      " [ 2 58 59  2 60 61 62 12 10 63 13 14 15 16 64 13 14 15 16 65  4 66]]\n"
     ]
    }
   ],
   "source": [
    "padded_sequences = pad_sequences(sequences)\n",
    "print(sequences)\n",
    "print(' ')\n",
    "print(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17 18  3 19 20  4  6 21 22  7 23 24  0  0  0  0  0  0  0  0  0  0]\n",
      " [25 26 27 28 29 30 31  4  8 32 33 34  8 35 36 37  0  0  0  0  0  0]\n",
      " [ 9  3 10 38 39 40 41 42  9  3 43  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 7 11 11 44 45  5  5 46 47 48 49  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [50  6 12 51 52 53  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2 54  2 55  4  2 56  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 5  3 57  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2 58 59  2 60 61 62 12 10 63 13 14 15 16 64 13 14 15 16 65  4 66]]\n"
     ]
    }
   ],
   "source": [
    "padded_sequences = pad_sequences(sequences, padding = 'post')\n",
    "print(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3 19 20  4  6 21 22  7 23 24]\n",
      " [31  4  8 32 33 34  8 35 36 37]\n",
      " [ 3 10 38 39 40 41 42  9  3 43]\n",
      " [11 11 44 45  5  5 46 47 48 49]\n",
      " [50  6 12 51 52 53  0  0  0  0]\n",
      " [ 2 54  2 55  4  2 56  0  0  0]\n",
      " [ 5  3 57  0  0  0  0  0  0  0]\n",
      " [15 16 64 13 14 15 16 65  4 66]]\n"
     ]
    }
   ],
   "source": [
    "# maxlen will truncate the characters where padding has been done, default = 'pre'\n",
    "# truncating will truncate the sentence, default = 'pre' means \n",
    "# words will be truncated from beginning\n",
    "\n",
    "padded_sequences = pad_sequences(sequences, padding = 'post',\n",
    "                                maxlen = 10, truncating = 'pre')\n",
    "print(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is anthony gonzalves and i am alone in this world',\n",
       " 'two and you three even then you returned empty handed',\n",
       " 'is not only difficult to capture don it is impossible',\n",
       " 'big big cities such small small incidences keep on happening',\n",
       " 'dog i will drink your blood <UNK> <UNK> <UNK> <UNK>',\n",
       " 'the good the bad and the ugly <UNK> <UNK> <UNK>',\n",
       " 'small is beautiful <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>',\n",
       " 'cannot learn but those who cannot learn unlearn and relearn']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(padded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Dataset (Reviews from Movie Subscribing Channel - IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q tensorflow-datasets  -- installed using conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "imdb, info = tfds.load(\"imdb_reviews\", with_info = True, as_supervised = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
